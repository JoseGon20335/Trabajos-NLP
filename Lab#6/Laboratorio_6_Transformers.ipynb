{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkCj1CZYDPVS"
      },
      "source": [
        "NOMBRE:\n",
        "\n",
        "\n",
        "CARNE:\n",
        "\n",
        "\n",
        "FECHA:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN22MYvxDrBp"
      },
      "source": [
        "Responda las siguientes preguntas:\n",
        "1. ¿Cuáles son los dos procesos principales de un transformer y qué función cumplen?\n",
        "  \n",
        "\n",
        "- A. Codificación (Encoder): El proceso de codificación toma la entrada y la convierte en una representación de alto nivel (un conjunto de características), conservando relaciones entre las palabras gracias al mecanismo de atención.\n",
        "\n",
        "- B. Decodificación (Decoder): El proceso de decodificación toma la representación codificada y la convierte en la salida deseada (por ejemplo, una traducción). También utiliza atención, tanto hacia la entrada codificada como hacia la propia salida parcial.\n",
        "\n",
        "\n",
        "  \n",
        "2. Asigne cada uno de las siguientes subtareas a su proceso correspondiente(A o B): Multi-Head Self-Attention Mechanism,Position-wise Feed-Forward Networks,Masked Multi-Head Self-Attention Mechanism, Encoder-Decoder Multi-Head Attention,Position-wise Feed-Forward Networks\n",
        "\n",
        "Después de asignar, explique la función de cada una de las partes. Luego, proceda a programar un transformer con cada una de esas partes utilizando solo tensorflow o Pytorch.\n",
        "\n",
        "\n",
        "**Asignación de las subtareas a su proceso correspondiente:**\n",
        "\n",
        "- Multi-Head Self-Attention Mechanism: Se encuentra en ambos procesos (Encoder y Decoder) y permite que el modelo se enfoque en diferentes partes de la entrada o la salida de manera simultánea.\n",
        "\n",
        "- Position-wise Feed-Forward Networks: También está presente en ambos (Encoder y Decoder). Se aplica después del mecanismo de atención para capturar patrones no lineales.\n",
        "\n",
        "- Masked Multi-Head Self-Attention Mechanism: Se encuentra solo en el Decodificador. Enmascara futuras posiciones para evitar que el modelo vea respuestas futuras durante el entrenamiento.\n",
        "\n",
        "- Encoder-Decoder Multi-Head Attention: Solo está presente en el Decodificador y permite que el decodificador se centre en partes relevantes de la entrada.\n",
        "\n",
        "**Explicación de las partes:**\n",
        "\n",
        "- Multi-Head Self-Attention: El modelo puede prestar atención a múltiples partes de la entrada al mismo tiempo, lo que ayuda a capturar relaciones complejas.\n",
        "\n",
        "- Position-wise Feed-Forward Networks: Se aplica de manera independiente en cada posición y capa para capturar interacciones no lineales entre las características.\n",
        "\n",
        "- Masked Multi-Head Self-Attention: Evita que el decodificador vea respuestas futuras durante el entrenamiento, asegurando una predicción secuencial.\n",
        "\n",
        "- Encoder-Decoder Multi-Head Attention: Permite que el decodificador use la información de la representación generada por el codificador para generar la salida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "B3tVVmNYDqWA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model) // 2)) / np.float32(d_model))\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    return tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0bYNL3FHBTM"
      },
      "source": [
        "Responda: ¿Qué es el posicional encoding y por qué debe definirse una función para los transformer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EShmMnX_DTMw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Definimos el TransformerBlock\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = self.point_wise_feed_forward_network(d_model, dff)\n",
        "        \n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "    \n",
        "    def call(self, x, mask, training=False):\n",
        "        attn_output, _ = self.att(x, x, x, mask)  # Multi-head attention\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # Residual connection + normalization\n",
        "        \n",
        "        ffn_output = self.ffn(out1)  # Feed-forward network\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection + normalization\n",
        "        return out2\n",
        "    \n",
        "    def point_wise_feed_forward_network(self, d_model, dff):\n",
        "        return tf.keras.Sequential([\n",
        "            Dense(dff, activation='relu'),  # First layer of FFN\n",
        "            Dense(d_model)  # Second layer of FFN\n",
        "        ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IWzapD7QHYyr"
      },
      "outputs": [],
      "source": [
        "# Multi-Head Attention class as defined before\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        assert d_model % self.num_heads == 0\n",
        "        \n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = Dense(d_model)\n",
        "        self.wk = Dense(d_model)\n",
        "        self.wv = Dense(d_model)\n",
        "        self.dense = Dense(d_model)\n",
        "    \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Divide la última dimensión en múltiples cabezas.\"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "        \n",
        "        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        \n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        \n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        return output, attention_weights\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
        "        \"\"\"Calcula la atención escalarizada.\"\"\"\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "        \n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "        \n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0qMe7xfaHY01"
      },
      "outputs": [],
      "source": [
        "# Clase completa del Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.encoder = [TransformerBlock(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.final_layer = Dense(target_vocab_size)  # Proyección al vocabulario de salida\n",
        "\n",
        "    def call(self, x, mask=None, training=False):\n",
        "        for encoder_layer in self.encoder:\n",
        "            x = encoder_layer(x, mask, training=training)  # Pasamos por las capas del encoder\n",
        "        \n",
        "        # Proyectar la salida final al tamaño del vocabulario\n",
        "        final_output = self.final_layer(x)  # (batch_size, seq_len, target_vocab_size)\n",
        "        return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "F71jNfpEHY3P"
      },
      "outputs": [],
      "source": [
        "#Datos de prueba\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "dff = 512\n",
        "input_vocab_size = 8500\n",
        "target_vocab_size = 8000\n",
        "pe_input = 1000\n",
        "pe_target = 1000\n",
        "dropout_rate = 0.1\n",
        "\n",
        "#Su output shape debe ser: (64, 50, 8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "a-b0nbXyHa2P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 50, 8000)\n"
          ]
        }
      ],
      "source": [
        "# Crear una entrada de prueba\n",
        "sample_input = tf.random.uniform((64, 50, d_model))  # (batch_size, seq_len, d_model)\n",
        "\n",
        "# Crear el transformer completo y obtener la salida\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, dropout_rate)\n",
        "sample_output = transformer(sample_input)\n",
        "\n",
        "print(sample_output.shape) "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
