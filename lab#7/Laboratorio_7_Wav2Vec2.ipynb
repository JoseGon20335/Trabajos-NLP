{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H93JXt0PFfjV"
      },
      "source": [
        "NOMBRE: Jose Miguel Gonzalez\n",
        "\n",
        "\n",
        "CARNE: 20335\n",
        "\n",
        "FECHA: 9/29/2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHa6i6K5Odqf"
      },
      "source": [
        "**Comentario General:** El objetivo de esta práctica es que exploren la arquitectura transformer que ya debieron haber leído en el paper original.\n",
        "\n",
        "Cuando lleguen a la parte de entrenamiento y evaluación, procuren obtener el mejor resultado para WER. Cambien parametros, congelen capas convolucionales, etc. Dejen evidencia de todos sus intentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Responda las siguientes preguntas después de haber leído el paper original de Wav2Vec2(Se encuentra en: Materiales del profesor/papers/transformers y LLM):\n",
        "¿Qué es Wav2Vec2 y qué problema intenta resolver?\n",
        "¿Cómo se diferencia Wav2Vec2 de los métodos tradicionales de reconocimiento de voz?\n",
        "¿Cuáles son los componentes principales de la arquitectura de Wav2Vec2?\n",
        "¿Puedes explicar el proceso de entrenamiento de Wav2Vec2? ¿Cómo funciona el aprendizaje auto-supervisado en este contexto?\n",
        "¿Qué papel juega la cuantización de características de audio en Wav2Vec2?\n",
        "¿Cuáles son las ventajas y desventajas de usar Wav2Vec2 en comparación con otros modelos de reconocimiento de voz?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ETek0w1Fl-u"
      },
      "outputs": [],
      "source": [
        "#Librerias a instalar, en caso de no estar instaladas\n",
        "\n",
        "#!pip install datasets\n",
        "#!pip install transformers\n",
        "#!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Hrc5UB0FwE6"
      },
      "outputs": [],
      "source": [
        "#Dataset con el que estarán trabajando\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "timit = load_dataset(\"timit_asr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILd1XTGcF3zN"
      },
      "outputs": [],
      "source": [
        "#Elimine las columnas innecesarias(para este caso didactico). Solo necesita \"text\",\"audio\",\"file\" y \"id\".\n",
        "\n",
        "timit = timit.remove_columns([col for col in timit[\"train\"].column_names if col not in [\"text\", \"audio\", \"file\", \"id\"]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG_NKDv3GVKj"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "#defina una función que reciba el dataset y in int \"n\". La función debe mostrar un dataframe \"n\" textos del dataset.\n",
        "\n",
        "def mostrar_textos(dataset, n):\n",
        "    df = pd.DataFrame(dataset[\"train\"][\"text\"])\n",
        "    return df.sample(n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCUBGjwmG5OP"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "#Limpie los textos.\n",
        "def limpiar_texto(texto):\n",
        "    texto = re.sub(r\"[^a-zA-Z\\s]\", \"\", texto)  # Eliminar caracteres especiales\n",
        "    texto = texto.lower()  # Convertir a minúsculas\n",
        "    return texto\n",
        "timit = timit.map(lambda x: {\"text\": limpiar_texto(x[\"text\"])})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaZyxP0FH8UO"
      },
      "outputs": [],
      "source": [
        "#Defina una funcion para construir el vocabulario. Esto es todos los caracteres que están en nuestro corpus.\n",
        "#Ejemplo: {a: 3, b:5, c:7}\n",
        "#Luego, cambie el espacio \" \" por \"|\". Esto es una buena practica, para tener un elemento visible para el espacio.\n",
        "#Al vocabulario debe agregar \"[UNK]\" y \"[PAD]\". Uno es para desconocidos, otro es para en blanco. Estos deben tener asignada\n",
        "#la longitud del vocabulario.\n",
        "#Guarde el vocabulario en un json: vocab.json\n",
        "\n",
        "def construir_vocabulario(dataset):\n",
        "    vocabulario = {}\n",
        "    for texto in dataset[\"train\"][\"text\"]:\n",
        "        for char in texto:\n",
        "            if char not in vocabulario:\n",
        "                vocabulario[char] = 1\n",
        "            else:\n",
        "                vocabulario[char] += 1\n",
        "    vocabulario[\"|\"] = vocabulario.get(\" \", 0)  # Reemplaza el espacio con \"|\"\n",
        "    vocabulario[\"[UNK]\"] = len(vocabulario) + 1\n",
        "    vocabulario[\"[PAD]\"] = len(vocabulario) + 1\n",
        "    return vocabulario\n",
        "\n",
        "vocabulario = construir_vocabulario(timit)\n",
        "\n",
        "import json\n",
        "with open('vocab.json', 'w') as vocab_file:\n",
        "    json.dump(vocabulario, vocab_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2HdlPAGJT-R"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2CTCTokenizer\n",
        "#Use el vocabulario anterior para tokenizar con Wav2Vec2TCTokenizer\n",
        "\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\"vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPTs69QKMAIj"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "#Defina el pipeline de extraccion de caracteristicas\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRHb_SegMET9"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "#Defina el pipeline de procesamiento\n",
        "\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec_d0SfiMN8c"
      },
      "outputs": [],
      "source": [
        "#Antes de empezar, evalue que todo esta bien: escuche un par de audios y vea si el texto de referencia es correcto\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRxkJrERMWN_"
      },
      "outputs": [],
      "source": [
        "#Use la siguiente funcion para crear los batches en el formato que el modelo necesita\n",
        "def prepare_dataset(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # batched output is \"un-batched\" to ensure mapping is correct\n",
        "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
        "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
        "\n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y39RAVnRM-hb"
      },
      "outputs": [],
      "source": [
        "#Training enviroment: use este como guía, puede que varíe respecto a como nombraron sus variables anteriormente.\n",
        "import torch\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnG9oNUyNM5r"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx4lEAdsOGpi"
      },
      "outputs": [],
      "source": [
        "#Metrica de rendimiento WER\n",
        "wer_metric = load_metric(\"wer\")\n",
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0ZMFnEOOORO"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2ForCTC\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    ctc_loss_reduction=\"mean\",\n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxoi-fS1OOvs"
      },
      "outputs": [],
      "source": [
        "#Parametros de entrenamiento segun la documentacion\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=repo_name,\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=8,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=30,\n",
        "  fp16=True,\n",
        "  gradient_checkpointing=True,\n",
        "  save_steps=500,\n",
        "  eval_steps=500,\n",
        "  logging_steps=500,\n",
        "  learning_rate=1e-4,\n",
        "  weight_decay=0.005,\n",
        "  warmup_steps=1000,\n",
        "  save_total_limit=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHB2Ke96OYeJ"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=timit[\"train\"],\n",
        "    eval_dataset=timit[\"test\"],\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "timit = timit.map(prepare_dataset, remove_columns=timit.column_names, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "repo_name = \"wav2vec2_asr_timit\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
