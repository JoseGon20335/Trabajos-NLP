{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXhjY9dGwJls"
      },
      "source": [
        "NOMBRES: Jose Miguel\n",
        "\n",
        "APELLIDOS: Gonzalez y Gonzalez\n",
        "\n",
        "CARNE: 20335\n",
        "\n",
        "FECHA: 8/23/2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7nnvCPLh5Ms"
      },
      "source": [
        "**Ejercicio 1**\n",
        "Con los datos, cálcule PPMI, pero aplicando Lapace Smoothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3aTZXJeiFAI"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf3GX1_liFXd"
      },
      "outputs": [],
      "source": [
        "# Renombrar columnas para mayor claridad\n",
        "data.columns = [\"Term\"] + list(data.iloc[0, 1:])\n",
        "data = data[1:]\n",
        "\n",
        "# Convertir los valores numéricos a enteros para realizar los cálculos\n",
        "for col in data.columns[1:]:\n",
        "    data[col] = pd.to_numeric(data[col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "# Calcular las frecuencias totales por palabra (sumando filas y columnas)\n",
        "total_word_frequency = data.set_index(\"Term\").sum(axis=1) + data.drop(columns=[\"Term\"]).sum(axis=0)\n",
        "\n",
        "# Calcular el total de todas las coocurrencias\n",
        "total_cooccurrences = data.drop(columns=[\"Term\"]).values.sum()\n",
        "\n",
        "# Calcular la probabilidad conjunta y aplicar Laplace Smoothing\n",
        "laplace_k = 1  # Smoothing factor\n",
        "vocab_size = len(total_word_frequency)  # Tamaño del vocabulario\n",
        "\n",
        "# Crear una matriz de probabilidades suavizadas\n",
        "ppmi_matrix = pd.DataFrame(index=data[\"Term\"], columns=data.columns[1:])\n",
        "\n",
        "for term1 in ppmi_matrix.index:\n",
        "    for term2 in ppmi_matrix.columns:\n",
        "        cooccurrence = data.loc[data[\"Term\"] == term1, term2].values[0]\n",
        "        # Probabilidad conjunta suavizada\n",
        "        p_xy = (cooccurrence + laplace_k) / (total_cooccurrences + laplace_k * vocab_size ** 2)\n",
        "        # Probabilidad marginales suavizadas\n",
        "        p_x = (total_word_frequency[term1] + laplace_k * vocab_size) / (total_cooccurrences + laplace_k * vocab_size ** 2)\n",
        "        p_y = (total_word_frequency[term2] + laplace_k * vocab_size) / (total_cooccurrences + laplace_k * vocab_size ** 2)\n",
        "        \n",
        "        # Calcular PMI\n",
        "        pmi = np.log2(p_xy / (p_x * p_y)) if p_x * p_y > 0 else 0\n",
        "        \n",
        "        # Calcular PPMI\n",
        "        ppmi_matrix.loc[term1, term2] = max(pmi, 0)\n",
        "\n",
        "import ace_tools as tools; tools.display_dataframe_to_user(name=\"PPMI con Laplace Smoothing\", dataframe=ppmi_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzRf6sNDiF1v"
      },
      "source": [
        "**Ejercicio 2**\n",
        "\n",
        "POC para crear información de entreno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f16xWrpDiH-9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o3CzZjlIiPA2"
      },
      "outputs": [],
      "source": [
        "#Librerías que necesitarán\n",
        "import io\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import utils\n",
        "%load_ext tensorboard\n",
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e0LLReSPiVxw"
      },
      "outputs": [],
      "source": [
        "sentece = \"The wind Crosses the brown land unheard\" #De mis poemas fav. The waste land de T.S Elliot\n",
        "tokens = #tokenizar\n",
        "vocab, index = #arreglo vocabulario, empezar indice en 1\n",
        "vocab[\"algo importante para que no explote el programa\"] = 0\n",
        "\n",
        "#Llene el arreglo para vocabulario\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "example_sequence = [vocab[word] for word in tokens]\n",
        "window_size = 2\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(#rellene esta area como le sea pertinente)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loA64ZR0k3L0",
        "outputId": "a8e28e58-e1bd-492c-c92b-13539c7a74c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5, 4): (land, brown)\n",
            "(5, 6): (land, unheard)\n",
            "(1, 2): (the, wind)\n",
            "(6, 5): (unheard, land)\n",
            "(3, 4): (crosses, brown)\n"
          ]
        }
      ],
      "source": [
        "#Resultados\n",
        "inverse_vocab = {index: token for token, index in vocab.items()}\n",
        "for target, context in positive_skip_grams[:5]:\n",
        "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdtE3R10lcAG"
      },
      "outputs": [],
      "source": [
        "target_word, context_word = positive_skip_grams[0]\n",
        "# Escoja un numero para muestras negativos (que no pertenence al contexto)\n",
        "num_ns =\n",
        "\n",
        "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
        "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "    true_classes=  # clase que debe ser muestreada como positiva (que pertenece al contexto)\n",
        "    num_true=1,\n",
        "    num_sampled=num_ns,\n",
        "    unique=True,\n",
        "    range_max=vocab_size,  # [0,vocab_size]\n",
        "    seed=SEED,\n",
        "    name=\"negative_sampling\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcxIUEmpmUt7"
      },
      "outputs": [],
      "source": [
        "print(negative_sampling_candidates)\n",
        "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpVHwBkDAurO"
      },
      "outputs": [],
      "source": [
        "sampling_table = tf.keras.preprocessing.sequence. #funcion que construye la tabla de muestreo en forma de frecuencias\n",
        "print(sampling_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3iK0LesA5Lu"
      },
      "outputs": [],
      "source": [
        "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1) #se agrega una dimension para poder concatenar\n",
        "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "target = tf.squeeze(target_word)\n",
        "context = tf.squeeze(context)\n",
        "label = tf.squeeze(label)\n",
        "\n",
        "#Listo! asi se prepara la info para entrenar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNHYhsn6D-jS",
        "outputId": "6dee158e-2219-440c-f8bd-62d824c1489b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "#Informacion a utilizar:\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omu8ON00A_0H"
      },
      "outputs": [],
      "source": [
        "#Paso 1: cree una funcion que estandarice el texto como ya hemos hecho e incrustela en una capa de Tensorflow (TextVectorization)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y64v-qOFDhDj"
      },
      "outputs": [],
      "source": [
        "vectorize_layer.adapt(text_ds.batch(1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h5OvM9GDrRt"
      },
      "outputs": [],
      "source": [
        "#Paso 2: Usando la POC de creacion de datos de entreno, defina una funcion para este proceso\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMmxeZPbEMOg"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
        "sequences = list(text_vector_ds.as_numpy_iterator())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsgvY5hZE5X4"
      },
      "outputs": [],
      "source": [
        "#El siguiente codigo es el modelo Word2Vec usando lo que ya han hecho. Sin embargo, deben agregar la metrica de similitud que vimos en clase, a la cual deben llamar dots\n",
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = layers.Embedding(vocab_size,\n",
        "                                      embedding_dim,\n",
        "                                      name=\"w2v_embedding\")\n",
        "    self.context_embedding = layers.Embedding(vocab_size,\n",
        "                                       embedding_dim)\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
        "    # context: (batch, context)\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "    # target: (batch,)\n",
        "    word_emb = self.target_embedding(target)\n",
        "    # word_emb: (batch, embed)\n",
        "    context_emb = self.context_embedding(context)\n",
        "\n",
        "\n",
        "    # dots: (batch, context)\n",
        "    return dots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCX5lVLdFN-D"
      },
      "outputs": [],
      "source": [
        "#Entrenar modelo y definir CategoricalCrossEntropy como funcion de perdida. Mostrar el print de accuracy y loss. Si deciden presentar sus resultados en tensorboard, adjuntar capturas."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
